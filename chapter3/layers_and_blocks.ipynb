{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 继承Block类构造模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd \n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Block):\n",
    "    # 申明模型的层\n",
    "    def __init__(self,**kwargs):\n",
    "        #调用父类构造函数进行必要的初始化\n",
    "        super(MLP,self).__init__(**kwargs)\n",
    "        # 隐含层\n",
    "        self.hidden = nn.Dense(256, activation='relu')\n",
    "        # 输出层\n",
    "        self.output = nn.Dense(10)\n",
    "\n",
    "    # 定义模型的输出，即根据输入x计算返回的所需的模型输出\n",
    "    def forward(self, x):\n",
    "        return self.output(self.hidden(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP()\n",
    "net.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[ 0.02394443  0.05150647  0.01035163 -0.06435341 -0.05801985  0.064192\n   0.04472664 -0.01852541 -0.03237379  0.07389369]\n [ 0.05207362  0.04186264  0.04021508 -0.06558423 -0.02249499  0.0341314\n   0.02135914 -0.06898528  0.02329672  0.0033668 ]]\n<NDArray 2x10 @cpu(0)>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(2,20))\n",
    "net(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sequential类继承自Block类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MySequential, self).__init__(**kwargs)\n",
    "\n",
    "    def add(self, block):\n",
    "        # bloc是Bloc子类的一个实例，将其保存在_children中，\n",
    "        # 其类型时OrderedDict\n",
    "        # 调用initialize时会自动初始化其中所有成员\n",
    "        self._children[block.name] = block \n",
    "\n",
    "    def forward(self, x):\n",
    "        # OrderedDict会保证按照添加时候的顺序遍历\n",
    "        for block in self._children.values():\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[-0.03358278  0.00098312  0.03334405 -0.00663612  0.07881726 -0.01704565\n  -0.01302506 -0.05449733  0.04149391  0.00170795]\n [ 0.01879605 -0.04185785  0.02918838 -0.00970372  0.05835275 -0.031299\n  -0.00644606 -0.02542868  0.0442826   0.01446365]]\n<NDArray 2x10 @cpu(0)>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "\n",
    "net.initialize()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构造复杂的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FancyMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(FancyMLP, self).__init__(**kwargs)\n",
    "        self.rand_weight = self.params.get_constant(\n",
    "            'rand_weight', nd.random.uniform(shape=(20, 20)))\n",
    "        self.Dense = nn.Dense(20, activation='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Dense(x)\n",
    "        x = nd.relu(nd.dot(x, self.rand_weight.data()) + 1)\n",
    "        x = self.Dense(x)\n",
    "        while x.norm().asscalar() > 1:\n",
    "            x /= 2\n",
    "        if x.norm().asscalar() > 0.8:\n",
    "            x *= 10\n",
    "        return x.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[37.89107]\n<NDArray 1 @cpu(0)>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FancyMLP()\n",
    "net.initialize()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NestMLP, self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential()\n",
    "        self.net.add(nn.Dense(64, activation='relu'), \n",
    "                        nn.Dense(32, activation='relu'))\n",
    "        self.Dense = nn.Dense(16, activation='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.Dense(self.net(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[1.4732156]\n<NDArray 1 @cpu(0)>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(NestMLP(), nn.Dense(20), FancyMLP())\n",
    "net.initialize()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型参数的共享和访问 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "\n",
    "net.initialize()\n",
    "\n",
    "X = nd.random.uniform(shape=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(dense30_ (\n   Parameter dense30_weight (shape=(256, 20), dtype=float32)\n   Parameter dense30_bias (shape=(256,), dtype=float32)\n ), mxnet.gluon.parameter.ParameterDict)"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].params,type(net[0].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(Parameter dense30_weight (shape=(256, 20), dtype=float32),\n Parameter dense30_weight (shape=(256, 20), dtype=float32))"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以是使用名字访问字典里的元素，也可以使用变量名如weight, bias\n",
    "net[0].params['dense30_weight'],net[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[-0.05052982  0.06649857 -0.01023339 -0.02788465  0.04799969 -0.05894421\n   0.04452466  0.03176292 -0.05566207  0.06655938 -0.04810633  0.03949251\n  -0.02741218 -0.03541367 -0.05944973 -0.06488146 -0.01054718 -0.01303093\n  -0.05493352  0.05288512]]\n<NDArray 1x20 @cpu(0)>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gluon使用parameter类，它包含参数值和梯度，可以使用data，grad函数访问\n",
    "net[0].weight.data()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n <NDArray 20 @cpu(0)>, \n [0.]\n <NDArray 1 @cpu(0)>)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z因为没有进行梯度的计算，所有梯度为0\n",
    "net[0].weight.grad()[0], net[1].bias.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "sequential6_ (\n  Parameter dense30_weight (shape=(256, 20), dtype=float32)\n  Parameter dense30_bias (shape=(256,), dtype=float32)\n  Parameter dense31_weight (shape=(10, 256), dtype=float32)\n  Parameter dense31_bias (shape=(10,), dtype=float32)\n)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以使用collect_params获取经过嵌套的层的所有参数\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init中有多种初始化权值的方法\n",
    "# 非首次对模型初始化需指定force_reinit为真\n",
    "net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用常数初始化\n",
    "net.initialize(init=init.Constant(1), force_reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n<NDArray 20 @cpu(0)>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对特定参数进行初始化，可以使用parameter类的初始化函数,与Block类相似\n",
    "net[0].weight.initialize(init= init.Normal(sigma=0.02), force_reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[-0.01331632 -0.01754761 -0.00382996  0.00997399  0.03486121 -0.01873092\n -0.00319623 -0.03353032  0.0006802  -0.02281556 -0.03817548  0.02163221\n  0.02128435 -0.00629699  0.04356062  0.01866355 -0.00596921 -0.0097684\n -0.02303199  0.02577024]\n<NDArray 20 @cpu(0)>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 自定义初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Init dense30_weight (256, 20)\nInit dense31_weight (10, 256)\n"
    },
    {
     "data": {
      "text/plain": "\n[-9.508459  -0.         0.        -6.8121257 -5.388574   0.\n -0.        -0.         6.0051117  8.644983   9.111368   5.773219\n -0.         9.755169   6.536106   7.747261  -7.920183  -9.8937\n  0.        -8.793293 ]\n<NDArray 20 @cpu(0)>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyInit(init.Initializer):\n",
    "    def _init_weight(self, name, data):\n",
    "        print('Init', name, data.shape)\n",
    "        data[:] = nd.random.uniform(low=-10, high=10, shape=data.shape)\n",
    "        data *= data.abs() >=5\n",
    "    \n",
    "net.initialize(MyInit(), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[-8.508459   1.         1.        -5.8121257 -4.388574   1.\n  1.         1.         7.0051117  9.644983  10.111368   6.773219\n  1.        10.755169   7.536106   8.747261  -6.920183  -8.8937\n  1.        -7.793293 ]\n<NDArray 20 @cpu(0)>"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以使用parameter类中set_data直接改写模型参数\n",
    "\n",
    "net[0].weight.set_data(net[0].weight.data()+1)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 共享参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[-6.1061524e-05 -2.4055375e-04  5.6250559e-05 -4.4983943e-05\n  -1.3011644e-04 -1.2308279e-05  2.3051508e-04  3.1897693e-04\n  -1.9086878e-04  1.8000347e-04]\n [-3.1810279e-05 -1.5045062e-04  1.6630591e-05 -4.1270498e-05\n  -1.0696284e-04  5.4987431e-06  1.3510580e-04  2.2468806e-04\n  -1.4344636e-04  1.2240042e-04]]\n<NDArray 2x10 @cpu(0)>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 除了在定义模型中多次调用同一层外，可以也可以添加层时进行指定\n",
    "# 下面定义二三层共享参数和梯度，二三层梯度会累加到shared.params.grad()中\n",
    "\n",
    "net = nn.Sequential()\n",
    "\n",
    "shared = nn.Dense(8, activation='relu')\n",
    "\n",
    "net.add(nn.Dense(8,activation='relu'),\n",
    "        shared,\n",
    "        nn.Dense(8, activation='relu', params=shared.params),\n",
    "        nn.Dense(10))\n",
    "net.initialize()\n",
    "\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\n[1. 1. 1. 1. 1. 1. 1. 1.]\n<NDArray 8 @cpu(0)>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].weight.data()[0] == net[2].weight.data()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 延后初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型定义后，因为没有给定输入的形状，就算指定了初始化的方法，也无法进行初始化，\n",
    "#但初始化一定会在前向传播前进行，这称为延后初始化\n",
    "\n",
    "#可以在定义模型时，指定每层的输入,nn.Dense(256,in_units=20),避免延后初始化\n",
    "#因为在延后初始化中，set_data，data函数无法使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}